---
title: "Trabajo Práctico Final"
author: "Gazze Simón, Moresi Manuel"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    self_contained: true
---

```{=html}
<style>
.math.display {
  font-size: 85%;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", warning = FALSE, message = FALSE)
options(scipen = 999)
```

```{r}
library(readxl)
library(ggplot2)
library(tidyverse)
library(patchwork)
library(moments)
library(MASS)
library(kableExtra)
library(scales)
```

## Introducción

El margen mínimo de solvencia constituye uno de los pilares fundamentales en la supervisión y estabilidad financiera del mercado asegurador. Se trata de una exigencia regulatoria destinada a garantizar que las compañías de seguros cuenten con un respaldo económico suficiente para hacer frente a sus obligaciones futuras, incluso en escenarios adversos.

Determinar dicho margen con precisión es clave no solo para cumplir con las normativas vigentes, sino también para contribuir a la salud financiera de la entidad aseguradora, prevenir situaciones de Ruina Técnica, y garantizar la confianza de los asegurados.

En este trabajo, se calculará el margen mínimo de solvencia tal que la probabilidad de solvencia sea del 99% para una empresa aseguradora en el año 2024, en base a su cartera de pólizas y la información disponible sobre siniestros del año 2023. Para realizar dicho cálculo se tendrá que simular la distribución del número de siniestros en el año 2024 y la distribución de las cuantías individuales en dicho año.

## Análisis descriptivo

Para obtener un panorama real de una posible distribución de las cuantías individuales en el año 2024 en base a la información recolectada en el año 2023 se realizó un ajuste por inflación, utilizando la serie CER obtenida en la página del [Banco Central de la República Argentina](https://www.bcra.gob.ar/PublicacionesEstadisticas/Principales_variables_datos.asp?serie=3540&detalle=CER%A0(Base%2002/02/2002=1)). Dichas cuantías ajustadas se obtienen mediante la siguiente fórmula:

$$Cuantía_{2024} = \frac{CER(01/01/2024)}{CER(xx/xx/2023)} \times Cuantía_{2023}$$

A continuación se presenta la distribución de las cuantías actualizadas:

```{r}
df_cuantias <- read_excel("Trabajo Final 2024 Base de Datos .xlsx")

a = ggplot(df_cuantias, aes(x=`Cuantía actualizada`)) +
  geom_histogram(bins = 20, fill = "#D979A2", color = "black")+
  labs(title = "Distribución de las cuantías ajustadas", y = "Frecuencia", x = "Cuantías")+
  theme_minimal()+
  theme(plot.title = element_text(size = 10,face = "bold", hjust = 0.5))

b = ggplot(df_cuantias)+
  aes(x=`Cuantía actualizada`)+
  geom_density(color = "#A61C3C")+
  labs(title = "Densidad de las cuantías ajustadas", y = "Frecuencia", x = "Cuantías")+
  scale_x_continuous(breaks = c(0,4000000,8000000,12000000))+
  theme_minimal()+
  theme(plot.title = element_text(size = 10,face = "bold", hjust = 0.5))

a + b

```

Se observa una distribución muy asimétrica a la derecha, presentando algunos valores extremos cercanos a los 4 millones de pesos.

Para complementar este análisis, se agrega una tabla con las principales medidas descriptivas de dicha variable:

```{r}
# Crear función para formato moneda
formato_pesos <- dollar_format(prefix = "$", big.mark = ".", decimal.mark = ",", accuracy = 0.01)

# Calcular estadísticas descriptivas
resumen <- df_cuantias %>%
  summarise(
    `Cantidad de siniestros` = n(),
    `Media` = mean(`Cuantía actualizada`, na.rm = TRUE),
    `Mediana` = median(`Cuantía actualizada`, na.rm = TRUE),
    `Desvío estándar` = sd(`Cuantía actualizada`, na.rm = TRUE),
    `Mínimo` = min(`Cuantía actualizada`, na.rm = TRUE),
    `1er Cuartil (Q1)` = quantile(`Cuantía actualizada`, 0.25, na.rm = TRUE),
    `3er Cuartil (Q3)` = quantile(`Cuantía actualizada`, 0.75, na.rm = TRUE),
    `Máximo` = max(`Cuantía actualizada`, na.rm = TRUE),
    `Asimetría` = e1071::skewness(`Cuantía actualizada`, na.rm = TRUE),
    `Cuantía Total` =  sum(`Cuantía actualizada`, na.rm = TRUE)
  ) %>%
  t() %>%
  as.data.frame() %>%
  tibble::rownames_to_column("Medida") %>%
  rename(`Valor` = V1)

# Agregar formato de pesos solo a las medidas monetarias
resumen <- resumen %>%
  mutate(Valor = ifelse(
    Medida %in% c("Media", "Mediana", "Desvío estándar", "Mínimo", "1er Cuartil (Q1)", "3er Cuartil (Q3)", "Máximo", "Cuantía Total"),
    formato_pesos(as.numeric(Valor)),
    round(as.numeric(Valor), 2)
  ))

# Mostrar la tabla con estilo
resumen %>%
  kable("html", caption = "Resumen descriptivo de la variable Cuantía actualizada", align = "l") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"), position = "center") %>%
  row_spec(0, bold = TRUE, background = "#A61C3C", color = "white") %>%
  column_spec(1, bold = TRUE)
```

Además, se cuenta con la información de la cantidad de siniestros y de las cuantías totales para los años 2021, 2022 y 2023. Se presentan a continuación:

```{r}
df <- data.frame(
  Año = c(2021, 2022, 2023),
  Pólizas = c(24752, 25348, 25615),
  Siniestros = c(3023, 3581, 3431)
)

# Pasamos de ancho a largo
df_long <- pivot_longer(df, cols = c(Pólizas, Siniestros),
                        names_to = "Tipo", values_to = "Cantidad")

# Gráfico de barras apiladas
ggplot(df_long, aes(x = factor(Año), y = Cantidad, fill = Tipo)) +
  geom_bar(stat = "identity", color = "black", position = "dodge") +
  labs(title = "Pólizas y Siniestros por año",
       x = "Año", y = "Cantidad") +
  scale_fill_manual(values = c("Pólizas" = "#A61C3C", "Siniestros" = "#D979A2")) +
  theme_minimal() +
  theme(plot.title = element_text(size = 10,face = "bold", hjust = 0.5)) 
```

Se observan comportamientos similares en todos los años analizados.

Las proporciones de siniestros en cada año son las siguientes,

$$p_{2021} = \frac{3023}{24752} = 0.1221$$

$$p_{2022} = \frac{3581}{25348} = 0.1412$$

$$p_{2023} = \frac{3431}{25615} = 0.1339$$

Tal como se comentó anteriormente, se observan valores similares respecto a la proporción de siniestros por póliza en los años analizados, por lo que se podría suponer que en el año 2024 se va a mantener la cantidad de pólizas y el perfil de las mismas.

## Desarrollo

Para realizar el cálculo del Margen de Solvencia Mínimo se debe obtener la distribución aproximada de la cuantía total de los siniestros en el año 2024. Para obtener dicha distribución hay que calcular (también mediante aproximaciones) las distribuciones de:

-   **Número de siniestros ocurridos**, en base a la información de la cantidad de siniestros ocurridos en los años 2021, 2022 y 2023.

-   **Cuantías individuales de los siniestros**, en base a la distribución ya conocida de las cuantías individuales en el año 2023.

Es decir, se simulará inicialmente el número de siniestros para un año y para esta cantidad de siniestros se simulará la cuantía de cada uno de ellos. Este procedimiento se repetirá para un gran número de años, peritiendo obtener así la distribución aproximada de la cuantía total.

Es importante aclarar que para cada uno de estos años simulados, se va a suponer que el número de pólizas es el mismo que las que se tienen en el año 2023, es decir, 25.615 pólizas.

### Propuesta 1 (Poisson - Log Normal)

Para esta primer propuesta, se utilizará la distribución de Poisson para simular el número de siniestros en 100.000 años, para posteriormente calcular la distribución de las cuantías individuales en cada uno de esos años.

En relación a lo visto anteriormente, el parámetro de la distribución Poisson ($E(N) = \lambda$, donde N es la cantidad de siniestros por póliza) será $\lambda = 0.1325$, el cual es el promedio de siniestros por póliza en los 3 años analizados.

```{r}
set.seed(123)
siniestros1 = numeric(100000)

if(FALSE){for(i in 1:100000){ 
siniestros1[i] <- sum(rpois(n = 25615, lambda = 0.1325))

}}

#saveRDS(siniestros1, file = "siniestros1.rds")
siniestros1 <- readRDS("siniestros1.rds")


if(FALSE){df_siniestros1 <- bind_rows(
  lapply(seq_along(siniestros1), function(i) {
    data.frame(valor = siniestros1[[i]])
  })
)}

#saveRDS(df_siniestros1, file = "df_siniestros1.rds")
df_siniestros1 <- readRDS("df_siniestros1.rds")

ggplot(df_siniestros1)+
  aes(x=valor)+
  geom_histogram(bins = 20, color = "black", fill = "#D979A2")+
  labs(title = "Distribución de las cantidades de siniestros simuladas", y = "Frecuencia", x = "Siniestros")+
  theme_minimal()+
  theme(plot.title = element_text(size = 10,face = "bold",hjust = 0.5))

```

Se observa una distribución simétrica, con un valor medio de 3394 siniestros y un desvío estándar de 58 siniestros.

Ahora se calculará la cuantía individual de cada uno de los siniestros en cada año, a partir de una distribución Log Normal, con $\hat{\mu} = 12,95207$ y $\hat{\sigma_x} = 0,3617761$. Dichas estimaciones son las *estimaciones máximo verosímiles* de esta distribución y se obtienen de la siguiente manera:

$$\hat{\mu} = \frac{1}{n}\sum_{i=1}^n log(x_i) = 12,95207 \ ,\ \hat{\sigma}= \frac{1}{n}\sum_{i=1}^n (log(x_i)-\hat{\mu})^2= 0,3617761$$

```{r}
set.seed(1)

log_media <- mean(log(df_cuantias$`Cuantía actualizada`))
log_desvio <- sd(log(df_cuantias$`Cuantía actualizada`))

simulaciones <- vector("list", length(siniestros1))

if(FALSE){for(i in 1:length(siniestros1)) {
  simulaciones[[i]] <- rlnorm(siniestros1[i], meanlog = log_media, sdlog = log_desvio)
}}

#muestra_100 <- sample(simulaciones, size = 100)

#saveRDS(muestra_100, file = "muestra_100.rds")

muestra_100 <- readRDS("muestra_100.rds")

df_muestra <- bind_rows(
  lapply(seq_along(muestra_100), function(i) {
    data.frame(valor = muestra_100[[i]], grupo = paste0("Dist_", i))
  })
)

ggplot(df_muestra, aes(x = valor, group = grupo)) +
  geom_density(size = 0.8, alpha = 0.7, color = "#A61C3C") +
  labs(title = "Densidades de 100 distribuciones Log-Normal simuladas",
       x = "Valor simulado", y = "Densidad") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10,face = "bold", hjust = 0.5)) 
```

Una vez obtenidas las cuantías individuales para cada año simulado, se sumarán dichas cuantías para obtener valores de 100.000 cuantías totales, formando así una distribución simulada de la misma:

```{r}
#totales <- data.frame(totales = sapply(simulaciones, sum))

#saveRDS(totales, file = "totales.rds")

totales <- readRDS("totales.rds")

a1 = ggplot(totales)+
  aes(x=totales)+
  geom_histogram(bins = 20, color = "black", fill = "#D979A2")+
  labs(title = "Distribución de las cuantías totales simuladas", y = "Frecuencia", x = "Cuantías totales")+
  theme_minimal()+
  theme(plot.title = element_text(size = 9,face = "bold"),axis.text.x = element_text(size = 8))

b1 = ggplot(totales)+
  aes(x=totales)+
  geom_density(color = "#A61C3C")+
  labs(title = "Densidad de las cuantías totales simuladas", y = "Frecuencia", x = "Cuantías totales")+
  theme_minimal()+
  theme(plot.title = element_text(size = 9,face = "bold"),axis.text.x = element_text(size = 8))

a1 + b1

#skewness(totales$totales)
#mean(totales$totales)
#sd(totales$totales)
```

La distribución simulada presenta un comportamiento parecido a la Normal, y al calcular el coeficiente de asimetría muestral se obtiene un valor de $0.038$. Por este motivo, el cálculo del Margen de Solvencia Mínimo se calculará mediante una aproximación Normal de las cuantías totales con $\mu = 1.528.104.408$ y $\sigma = 28.118.258$.


#### Margen de Solvencia Mínimo

Para este cálculo se va a tener que definir de antemano la prima recargada (PR) que se va a cobrar al cliente, con recargos de seguridad distintos, 1%, 2%, 3% y 4%.

Para que la probabilidad de solvencia sea del 99%, se necesita conocer el valor de la Normal Estandar que acumula el 99% de probabilidad.

$$ Z \sim  N(0,1)  \rightarrow P(Z < z_0) = 0,99 \rightarrow z_0 = 2,326$$

-   Para un *recargo de seguridad (R) del* $(\alpha \times 100)$%:

$$E(Y) = 1.528.104.408 , \ SD(Y) = 28.118.258 , \ R = E(Y) \times \alpha , \ PR = E(Y) + R $$

Entonces el monto total a cobrar al cliente para garantizar una solvencia del 99% es:

$$ y_0 = 1.528.104.408 + 2,326\times28.118.258 = 1.593.507.476 $$

Y el **Margen de Solvencia Mínimo** resulta:

$$MSM = y_0 - PR = 1.593.507.476 - PR$$
Para los distintos recargos entonces tenemos:

```{r}
msm_data <- data.frame(
  Porcentaje = c("1%", "2%", "3%", "4%"),
  PR = c(1543385452, 1558666496, 1573947540, 1589228584),
  MSM = c(50122024, 34840980, 19559936, 4278892)
)

msm_data_formateado <- msm_data %>%
  mutate(
    PR = dollar(PR, prefix = "$", big.mark = ".", decimal.mark = ","),
    MSM = dollar(MSM, prefix = "$", big.mark = ".", decimal.mark = ",")
  )

msm_data_formateado %>%
  kable("html", caption = "Margen de Solvencia Mínimo (MSM) según distintos porcentajes de recargo", align = "c") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"), position = "center") %>%
  row_spec(0, bold = TRUE, background = "#A61C3C", color = "white") %>%
  column_spec(1, bold = TRUE)
```

Se observa que a mayor porcentaje de recargo, menor será el Margen de Solvencia Mínimo que obtendremos.

### Propuesta 2 (Poisson - Weibull)

Para esta propuesta, se utilizará nuevamente la distribución de Poisson para simular el número de siniestros en 100.000 años, para posteriormente calcular la distribución de las cuantías individuales en cada uno de esos años.

Nuevamente, el parámetro de la distribución Poisson ($E(N) = \lambda$, donde N es la cantidad de siniestros por póliza) será $\hat{\lambda} = 0.1325$.

```{r}
set.seed(124)
#siniestros2 = numeric(100000)

if(FALSE){for(i in 1:100000){ 
siniestros2[i] <- sum(rpois(n = 25615, lambda = 0.1325))

}}

#saveRDS(siniestros2, file = "siniestros2.rds")
siniestros2 <- readRDS("siniestros2.rds")


if(FALSE){df_siniestros2 <- bind_rows(
  lapply(seq_along(siniestros2), function(i) {
    data.frame(valor = siniestros2[[i]])
  })
)}

#saveRDS(df_siniestros2, file = "df_siniestros2.rds")
df_siniestros2 <- readRDS("df_siniestros2.rds")

ggplot(df_siniestros2)+
  aes(x=valor)+
  geom_histogram(bins = 20, color = "black", fill = "#D979A2")+
  labs(title = "Distribución de las cantidades de siniestros simuladas", y = "Frecuencia", x = "Siniestros")+
  theme_minimal()+
  theme(plot.title = element_text(size = 10,face = "bold",hjust = 0.5))

```

Nuevamente se observa una distribución de los siniestros muy simétrica, con un valor medio de 3394 siniestros y un desvío estandar de 58 siniestros.

Ahora se calculará la cuantía individual de cada uno de los siniestros en cada año, a partir de una distribución de Weibull, con $\hat{\alpha} = 1,9831$ y $\hat{\beta} = 501.076,6639$. Dicha estimación de los parámetros se realizó mediante el *método de máxima verosimilitud*(el cálculo mencionado no se realiza con una formula simple, sino que debe recurrirse a métodos numéricos):

```{r}
set.seed(2)

fit <- fitdistr(df_cuantias$`Cuantía actualizada`, "weibull")

#simulaciones2 <- vector("list", length(siniestros2))

if (FALSE) {for(i in 1:length(siniestros2)) {
  simulaciones2[[i]] <- rweibull(siniestros2[i], shape = 1.98319396, scale = 501076.66390846)
}} 

#muestra2_100 <- sample(simulaciones2, size = 100)

#saveRDS(muestra2_100, file = "muestra2_100.rds")

muestra2_100 <- readRDS("muestra2_100.rds")

df_muestra2 <- bind_rows(
  lapply(seq_along(muestra2_100), function(i) {
    data.frame(valor = muestra2_100[[i]], grupo = paste0("Dist_", i))
  })
)

ggplot(df_muestra2, aes(x = valor, group = grupo)) +
  geom_density(size = 0.8, alpha = 0.7, color = "#A61C3C") +
  labs(title = "Densidades de 100 distribuciones Weibull simuladas",
       x = "Valor simulado", y = "Densidad") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10,face = "bold", hjust = 0.5)) 
```

Esta distribución simulada de las cuantías individuales presenta un comportamiento menos asimétrico que la distribución obtenida en la *Propuesta 1*, también se observa una cola menos pesada, es decir, no presenta valores tan extremos de las cuantías.

Una vez obtenidas las cuantías individuales para cada año simulado, se sumarán dichas cuantías para obtener valores de 100.000 cuantías totales, formando así una distribución simulada de la misma:

```{r}
#totales2 <- data.frame(totales2 = sapply(simulaciones2, sum))

#saveRDS(totales2, file = "totales2.rds")

totales2 <- readRDS("totales2.rds")

a2 = ggplot(totales2)+
  aes(x=totales2)+
  geom_histogram(bins = 20, color = "black", fill = "#D979A2")+
  labs(title = "Distribución de las cuantías totales simuladas", y = "Frecuencia", x = "Cuantías totales")+
  theme_minimal()+
  theme(plot.title = element_text(size = 9,face = "bold"), axis.text.x = element_text(size = 8))

b2 = ggplot(totales2)+
  aes(x=totales2)+
  geom_density(color = "#A61C3C")+
  labs(title = "Densidad de las cuantías totales simuladas", y = "Frecuencia", x = "Cuantías totales")+
  theme_minimal()+
  theme(plot.title = element_text(size = 9,face = "bold"), axis.text.x = element_text(size = 8))

a2 + b2

#skewness(totales2$totales2)
#mean(totales2$totales2)
#sd(totales2$totales2)
```

Al realizar las 100.000 simulaciones, se observa que la distribución resultante se asemeja a una distribición Normal, con un coeficiente de asimetría muestral igual a $0.012$. Por lo tanto, el Margen de Solvencia Mínimo se calculará mediante una aproximación Normal de las cuantías totales con $\mu = 1.507.429.712$ y $\sigma = 29.268.303$.

#### Margen de Solvencia Mínimo

Para ser consistentes con lo trabajado anteriormente, se utilizarán recargos de seguridad del 1% al 4%.

Para que la probabilidad de solvencia sea del 99%, se necesita conocer el valor de la Normal Estandar que acumula el 99% de probabilidad.

$$ Z \sim  N(0,1)  \rightarrow P(Z < z_0) = 0,99 \rightarrow z_0 = 2,326$$
-   Para un *recargo de seguridad (R) del* $(\alpha \times 100)$%:

$$E(Y) = 1.507.429.712 , \ SD(Y) = 29.268.303 , \ R = E(Y) \times \alpha , \ PR = E(Y) + R $$

Entonces el monto total a cobrar al cliente para garantizar una solvencia del 99% es:

$$ y_0 = 1.507.429.712 + 2,326\times29.268.303 = 1.575.507.785 $$

Y el **Margen de Solvencia Mínimo** resulta:

$$MSM = y_0 - PR = 1.575.507.785 - PR$$
Para los distintos recargos entonces tenemos:

```{r}
msm_data_1 <- data.frame(
  Porcentaje = c("1%", "2%", "3%", "4%"),
  PR = c(1522504009, 1537578306, 1552652603, 1567726900),
  MSM = c(53003776, 37929479, 22855182, 7780885)
)

msm_data_formateado_1 <- msm_data_1 %>%
  mutate(
    PR = dollar(PR, prefix = "$", big.mark = ".", decimal.mark = ","),
    MSM = dollar(MSM, prefix = "$", big.mark = ".", decimal.mark = ",")
  )

msm_data_formateado_1 %>%
  kable("html", caption = "Margen de Solvencia Mínimo (MSM) según distintos porcentajes de recargo", align = "c") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"), position = "center") %>%
  row_spec(0, bold = TRUE, background = "#A61C3C", color = "white") %>%
  column_spec(1, bold = TRUE)
```

### Propuesta 3 (Poisson - Gamma)

Al igual que en los casos anteriores, se simulará la distribución de la cantidad de siniestros mediante la distribución Poisson con parámetro $\hat{\lambda} = 0.1325$.

La distribución resultante es:

```{r}
set.seed(125)
#siniestros3 = numeric(100000)

if(FALSE){for(i in 1:100000){ 
siniestros3[i] <- sum(rpois(n = 25615, lambda = 0.1325))

}}

#saveRDS(siniestros3, file = "siniestros3.rds")
siniestros3 <- readRDS("siniestros3.rds")


if(FALSE){df_siniestros3 <- bind_rows(
  lapply(seq_along(siniestros3), function(i) {
    data.frame(valor = siniestros3[[i]])
  })
)}

#saveRDS(df_siniestros3, file = "df_siniestros3.rds")
df_siniestros3 <- readRDS("df_siniestros3.rds")

ggplot(df_siniestros3)+
  aes(x=valor)+
  geom_histogram(bins = 20, color = "black", fill = "#D979A2")+
  labs(title = "Distribución de las cantidades de siniestros simuladas", y = "Frecuencia", x = "Siniestros")+
  theme_minimal()+
  theme(plot.title = element_text(size = 10,face = "bold",hjust = 0.5))
```

Obteniendo una media en la cantidad de siniestros igual a 3394 siniestros, y un desvío estandar de 58 siniestros.

Ahora para el cálculo de las cuantías individuales, se opta por la distribución Gamma, dicha distribución es muy flexible y permite modelar distribuciones con distintos grados de asimetría. Para este caso los parámetros $\alpha$ y $\beta$ serán estimados de la siguiente manera:

$$\hat{\beta}  = \frac{S^2_x}{\overline{X}} = 119.944,1 \ ; \ \hat{\alpha} = \frac{\overline{X}}{\hat{\beta}} = 3,789819$$

```{r}
set.seed(3)

beta = var(df_cuantias$`Cuantía actualizada`)/mean(df_cuantias$`Cuantía actualizada`)

alpha = mean(df_cuantias$`Cuantía actualizada`)/beta

simulaciones3 <- vector("list", length(siniestros3))

if(FALSE){for(i in 1:length(siniestros3)) {
  simulaciones3[[i]] <- rgamma(siniestros3[i], shape = 3.789819, scale = 119944.1)
}}

#muestra3_100 <- sample(simulaciones3, size = 100)

#saveRDS(muestra3_100, file = "muestra3_100.rds")

muestra3_100 <- readRDS("muestra3_100.rds")

df_muestra3 <- bind_rows(
  lapply(seq_along(muestra3_100), function(i) {
    data.frame(valor = muestra3_100[[i]], grupo = paste0("Dist_", i))
  })
)

ggplot(df_muestra3, aes(x = valor, group = grupo)) +
  geom_density(size = 0.8, alpha = 0.7, color = "#A61C3C") +
  labs(title = "Densidades de 100 distribuciones Gamma simuladas",
       x = "Valor simulado", y = "Densidad") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10,face = "bold", hjust = 0.5)) 
```

Se observa una distribución bastante similar a las cuantías ajustadas observadas del año 2023, resultando un punto medio en relación a las 2 distribuciones simuladas anteriormente.

Mediante la suma de las cuantías en cada año se obtendrá la distribución de las cuantías totales, la misma resulta:

```{r}
#totales3 <- data.frame(totales3 = sapply(simulaciones3, sum))

#saveRDS(totales3, file = "totales3.rds")

totales3 <- readRDS("totales3.rds")

a3 = ggplot(totales3)+
  aes(x=totales3)+
  geom_histogram(bins = 20, color = "black", fill = "#D979A2")+
  labs(title = "Distribución de las cuantías totales simuladas", y = "Frecuencia", x = "Cuantías totales")+
  theme_minimal()+
  theme(plot.title = element_text(size = 9,face = "bold"), axis.text.x = element_text(size = 8))

b3 = ggplot(totales3)+
  aes(x=totales3)+
  geom_density(color = "#A61C3C")+
  labs(title = "Densidad de las cuantías totales simuladas", y = "Frecuencia", x = "Cuantías totales")+
  theme_minimal()+
  theme(plot.title = element_text(size = 9,face = "bold"), axis.text.x = element_text(size = 8))

a3 + b3

#skewness(totales3$totales3)
#mean(totales3$totales3)
#sd(totales3$totales3)
```

Nuevamente se observa una distribución muy simetrica, esto puede deberse a la elevada cantidad de simulaciones realizadas (100.000), donde observamos un coeficiente de asimetría muestral igual a $0.0219$.

Por lo tanto, y al igual que los casos anteriores, el Margen de Solvencia Mínimo se calculará mediante una aproximación Normal de las cuantías totales con $\mu = 1.542.747.461$ y $\sigma = 29.784.871$.

#### Margen de Solvencia Mínimo

Nuevamente, buscando ser consistentes con lo trabajado anteriormente, se utilizarán recargos de seguridad del 1% al 4%.

Para que la probabilidad de solvencia sea del 99%, se necesita conocer el valor de la Normal Estandar que acumula el 99% de probabilidad.

$$ Z \sim  N(0,1)  \rightarrow P(Z < z_0) = 0,99 \rightarrow z_0 = 2,326$$
-   Para un *recargo de seguridad (R) del* $(\alpha \times 100)$%:

$$E(Y) = 1.542.747.461 , \ SD(Y) = 29.784.871 , \ R = E(Y) \times \alpha , \ PR = E(Y) + R $$

Entonces el monto total a cobrar al cliente para garantizar una solvencia del 99% es:

$$ y_0 = 1.542.747.461 + 2,326\times29.784.871 = 1.612.027.071 $$

Y el **Margen de Solvencia Mínimo** resulta:

$$MSM = y_0 - PR = 1.612.027.071 - PR$$
Para los distintos recargos entonces tenemos:

```{r}
msm_data_2 <- data.frame(
  Porcentaje = c("1%", "2%", "3%", "4%"),
  PR = c(1558174936, 1573602410, 1589029885, 1604457359),
  MSM = c(53852135, 38424661, 22997186, 7569712)
)

msm_data_formateado_2 <- msm_data_2 %>%
  mutate(
    PR = dollar(PR, prefix = "$", big.mark = ".", decimal.mark = ","),
    MSM = dollar(MSM, prefix = "$", big.mark = ".", decimal.mark = ",")
  )

msm_data_formateado_2 %>%
  kable("html", caption = "Margen de Solvencia Mínimo (MSM) según distintos porcentajes de recargo", align = "c") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"), position = "center") %>%
  row_spec(0, bold = TRUE, background = "#A61C3C", color = "white") %>%
  column_spec(1, bold = TRUE)
```

## Discusión

### Distribución de la cantidad de siniestros

A la hora de elegir una distribución para simular la cantidad de siniestros por póliza ocurridos en un año, se pensaron inicialmente en 2 alternativas: Distribución *Poisson* y Distribución *Binomial Negativa*.

Es sabido que en los casos que no se cumple la igualdad entre la media y la variancia del número de siniestros, la distribución Poisson no resulta adecuada. Por lo tanto uno podría optar por una distribución más flexible que resulte útil en estos casos, es decir, la distribución Binomial Negativa.

En este caso, al solo contar con la información de número de siniestros totales y número de pólizas en los 3 años anteriores, no es posible identificar si el supuesto de igualdad entre media y variancia se cumple o no. Adicionalmente, como no se cuenta con la información del número de siniestros **por póliza** en años anteriores, no va a ser posible estimar de manera lógica los parámetros necesarios para obtener la distribución Binomial Negativa .

En base a todo lo mencionado anteriormente, se decide trabajar en todas las propuestas con una Distribución Poisson para modelar la cantidad de siniestros por póliza.

### Distribución de las cuantías individuales

Una de las cuestiones fundamentales a tener en cuenta a la hora de calcular el Margen de Solvencia Mínimo que debe garantizar la empresa aseguradora, es el de definir correctamente una distribución para las cuantías individuales.

En este trabajo se exploraron 3 posibilidades distintas, pero en ningún momento se realizaron comentarios con respecto a cual de estas se ajustaba mejor a los datos observados. A continuación se comparan las 3 distribuciones que se plantearon, junto a la distribución de las cuantías observadas ajustadas.

```{r}
# Empírica
dens_emp <- density(df_cuantias$`Cuantía actualizada`, na.rm = TRUE)
df_emp <- data.frame(x = dens_emp$x, y = dens_emp$y, distribucion = "Empírica")

# Secuencia común para las teóricas
x <- seq(0, 3000000, length.out = 100000)

# Log-Normal
dens_ln <- dlnorm(x, meanlog = log_media, sdlog = log_desvio)
df_ln <- data.frame(x = x, y = dens_ln, distribucion = "Log-normal")

# Weibull
dens_wb <- dweibull(x, shape = 1.98319396, scale = 501076.66390846)
df_wb <- data.frame(x = x, y = dens_wb, distribucion = "Weibull")

# Gamma
dens_gm <- dgamma(x, shape = 3.789819, scale = 119944.1)
df_gm <- data.frame(x = x, y = dens_gm, distribucion = "Gamma")

# Unir todos los data frames
df_teoricas <- bind_rows(df_ln, df_wb, df_gm)

# Graficar
ggplot() +
  # Fondo translucido de la empírica
  geom_area(data = df_emp, aes(x = x, y = y), fill = "#A61C3C", alpha = 0.4) +
  # Línea de la empírica
  geom_line(data = df_emp, aes(x = x, y = y, color = "Empírica"), size = 1) +
  # Líneas de densidades teóricas
  geom_line(data = df_teoricas, aes(x = x, y = y, color = distribucion), size = 1) +
  labs(title = "Comparación de densidades",
       x = "Cuantía",
       y = "Densidad",
       color = "Distribución") +
  coord_cartesian(xlim = c(0, 2000000)) +  
  theme_minimal() +
  theme(plot.title = element_text(size = 12, face = "bold", hjust = 0.5))

```

Claramente se observa que la distribución que mejor se ajusta a los datos observados es la **Distribución Log-Normal**.

### Márgenes de Solvencia Mínimo según distribución

Luego de calcular los Margenes de Solvencia Mínimos para cada una de las propuestas, se presentan dichos todos estos valores a continuación:

```{r}

tabla_msm <- tribble(
  ~Distribución,  ~`MSM (1%)`, ~`MSM (2%)`,  ~`MSM (3%)`,  ~`MSM (4%)`,
  "Log-normal", 50122024, 34840980, 19559936, 4278892,
  "Weibull", 53003776, 37929479, 22855182, 7780885,
  "Gamma", 53852135, 38424661, 22997186, 7569712
)

tabla_msm %>%
  mutate(across(starts_with("MSM"), ~ dollar(., prefix = "$", big.mark = ".", decimal.mark = ","))) %>%
  kable("html", caption = "Márgenes de Solvencia Mínimos (MSM) para distintos recargos y distribuciones") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  row_spec(0, bold = TRUE, background = "#A61C3C", color = "white")
```

Se puede identificar que los margenes varían para los distintos recargos, resultando que a mayor recargo de seguridad se obtiene un menor Margen de Solvencia Mínimo. Mientras que también se observa que dicho margen varía con respecto a las distintas distribuciones propuestas para las cuantías individuales, resultando la distribución Log-Normal la que presenta valores más chicos de dichos margenes, siendo a su vez, la que mejor ajusta a los datos observados.

Esto resultó de interés ya que, si se eligiera otra distribución entre las propuestas para simular las cuantías, se podría estar cobrandole más de lo necesario a nuestros clientes.
